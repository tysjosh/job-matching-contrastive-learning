# Career-Aware Contrastive Learning (CACL) â€” Methodology

## 1. Problem Statement

Resume-job matching is a semantic matching problem: given a resume and a job description, predict whether the candidate is a good fit (i.e., likely to receive an interview). Traditional keyword-based approaches fail to capture the nuanced relationships between career trajectories, transferable skills, and job requirements.

CACL addresses this by learning a shared embedding space where semantically compatible resume-job pairs are pulled closer together, while incompatible pairs are pushed apart â€” guided by occupational ontology knowledge from the ESCO (European Skills, Competences, Qualifications and Occupations) framework.

## 2. System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CACL Training Pipeline                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Structured   â”‚    â”‚  ESCO KG     â”‚    â”‚  Raw Dataset          â”‚  â”‚
â”‚  â”‚  Dataset      â”‚â”€â”€â”€â–¶â”‚  Enrichment  â”‚â”€â”€â”€â–¶â”‚  (JSONL)              â”‚  â”‚
â”‚  â”‚  (Resume+Job) â”‚    â”‚  (Skill URIs)â”‚    â”‚  4,143 train samples  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                       â”‚              â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                          â”‚   80/10/10 Data Split   â”‚ â”‚
â”‚                                          â”‚  Train / Val / Test     â”‚ â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                       â”‚              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚              â”‚
â”‚                    â”‚                                  â”‚â”‚              â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚              â”‚  Phase 1   â”‚                  â”‚     Phase 2        â”‚   â”‚
â”‚              â”‚ Contrastiveâ”‚â”€â”€â”€ best ckpt â”€â”€â–¶â”‚  Classification    â”‚   â”‚
â”‚              â”‚ Pretrainingâ”‚                  â”‚  Fine-tuning       â”‚   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â”‚                                  â”‚              â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚              â”‚  Phase 1   â”‚                  â”‚     Phase 2        â”‚   â”‚
â”‚              â”‚ Evaluation â”‚                  â”‚  Evaluation        â”‚   â”‚
â”‚              â”‚ (Val set)  â”‚                  â”‚  (Test set)        â”‚   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3. Dataset and Data Representation

### 3.1 Dataset Overview

The dataset consists of 5,179 resume-job pairs derived from real recruitment data. Each sample was originally labeled with one of three categories by domain experts:

| Original Label | Count | Description |
|---------------|-------|-------------|
| good_fit | 1,353 | Strong match â€” candidate likely to receive interview |
| potential_fit | 1,139 | Partial match â€” candidate has some relevant qualifications |
| no_fit | 2,687 | Poor match â€” candidate lacks key requirements |

For binary classification, these are mapped as: `good_fit â†’ 1 (positive)`, `potential_fit â†’ 0 (negative)`, `no_fit â†’ 0 (negative)`. This means the negative class contains both clearly unqualified candidates and borderline candidates, which introduces label noise and makes the classification task harder.

### 3.2 Data Splits

The dataset is split sequentially (80/10/10) to avoid temporal leakage:

| Split | Total | Positive (good_fit) | Negative | Pos % | Unique Jobs |
|-------|-------|---------------------|----------|-------|-------------|
| Train | 4,143 | 1,086 | 3,057 (920 potential + 2,137 no_fit) | 26.2% | 381 |
| Validation | 517 | 122 | 395 (107 potential + 288 no_fit) | 23.6% | 149 |
| Test | 519 | 145 | 374 (112 potential + 262 no_fit) | 27.9% | 155 |

### 3.3 ESCO Enrichment

Each sample is pre-enriched with ESCO ontology data during preprocessing. Skill names from resumes and jobs are mapped to ESCO skill URIs, enabling ontology-based distance computation. 78% of training samples have both resume and job skill URIs available.

Quality tier distribution (training set): Tier A: 77.6% | Tier B: 0.4% | Tier C: 22.0%

### 3.4 Sample Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Training Sample                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Resume            â”‚           Job                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ role                   â”‚ â€¢ title                           â”‚
â”‚ â€¢ experience_level       â”‚ â€¢ description                     â”‚
â”‚ â€¢ skills [{name, uri}]   â”‚ â€¢ required_skills [{name, uri}]   â”‚
â”‚ â€¢ experience [entries]   â”‚ â€¢ skill_uris (from ESCO)          â”‚
â”‚ â€¢ skill_uris (from ESCO) â”‚                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ label: 1 (positive/match) or 0 (negative/no match)          â”‚
â”‚ metadata: quality_tier, ontology_similarity, ot_distance     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Precomputed metadata fields:
- **ontology_similarity**: Symmetric best-match skill similarity between resume and job skill URI sets (0â€“1)
- **ot_distance**: Sinkhorn optimal transport distance between skill sets on the ESCO graph
- **quality_tier**: A/B/C grade based on ESCO skill URI coverage (A = both have rich skill URIs, C = one or both missing)

## 4. ESCO Knowledge Graph Integration

### 4.1 What is ESCO?

ESCO (European Skills, Competences, Qualifications and Occupations) is a standardized occupational taxonomy maintained by the European Commission. It provides a structured graph connecting occupations to the skills they require. CACL uses the ESCO knowledge graph to inject domain knowledge about skill relationships into the training process.

### 4.2 Graph Structure

The ESCO knowledge graph used in CACL contains 18,237 nodes and 132,679 edges:

| Node Type | Count | Description |
|-----------|-------|-------------|
| Skills | 14,359 | Individual skills/competences (e.g., "Python programming", "project management") |
| Occupations | 3,039 | Job roles (e.g., "Software developer", "Data analyst") |
| ISCO Groups | 619 | Hierarchical occupation categories from the ISCO-08 classification |
| Other | 220 | Additional classification nodes |

Edges connect occupations to the skills they require (`occupation â†’ skill`) and occupations to their ISCO group (`occupation â†’ isco`). When converted to an undirected graph, this creates implicit paths between skills: two skills are "close" if they are required by the same or similar occupations.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ESCO Knowledge Graph Structure                      â”‚
â”‚                                                                       â”‚
â”‚  Example: How "Python" and "Java" are connected                       â”‚
â”‚                                                                       â”‚
â”‚  skill:Python â—„â”€â”€â”€â”€â”€â”€ occupation:Software_Developer â”€â”€â”€â”€â”€â”€â–º skill:Javaâ”‚
â”‚       â”‚                        â”‚                                â”‚     â”‚
â”‚       â”‚                   isco:C2514                             â”‚     â”‚
â”‚       â”‚                  (Software                               â”‚     â”‚
â”‚       â”‚                  developers)                             â”‚     â”‚
â”‚       â”‚                        â”‚                                â”‚     â”‚
â”‚       â””â”€â”€â”€â”€ occupation:Data_Scientist â”€â”€â”€â”€â–º skill:Statistics â”€â”€â”€â”˜     â”‚
â”‚                                                                       â”‚
â”‚  shortest_path(Python, Java) = 2  (through Software_Developer)        â”‚
â”‚  shortest_path(Python, Statistics) = 2  (through Data_Scientist)      â”‚
â”‚  shortest_path(Java, Statistics) = 4  (through two occupations)       â”‚
â”‚                                                                       â”‚
â”‚  Skills required by the same occupation are distance 2 apart.         â”‚
â”‚  Skills in related occupations are further apart.                     â”‚
â”‚  Skills in unrelated fields have no path (distance = âˆ).             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 How CACL Uses the ESCO Graph

The ESCO graph is used in two distinct ways during training:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Two Uses of ESCO in CACL                        â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Usage 1: Negative          â”‚  â”‚  Usage 2: Sample-level      â”‚ â”‚
â”‚  â”‚  Selection (Â§5.6)           â”‚  â”‚  Loss Weighting (Â§5.7)      â”‚ â”‚
â”‚  â”‚                             â”‚  â”‚                             â”‚ â”‚
â”‚  â”‚  COMPUTED AT TRAINING TIME  â”‚  â”‚  PRECOMPUTED DURING         â”‚ â”‚
â”‚  â”‚                             â”‚  â”‚  DATA PREPROCESSING         â”‚ â”‚
â”‚  â”‚  For each anchor resume,    â”‚  â”‚                             â”‚ â”‚
â”‚  â”‚  compute skill similarity   â”‚  â”‚  Each sample stores:        â”‚ â”‚
â”‚  â”‚  to every candidate         â”‚  â”‚  â€¢ ontology_similarity      â”‚ â”‚
â”‚  â”‚  negative job. Bucket       â”‚  â”‚  â€¢ ot_distance              â”‚ â”‚
â”‚  â”‚  into hard/medium/easy      â”‚  â”‚  â€¢ quality_tier             â”‚ â”‚
â”‚  â”‚  based on distance.         â”‚  â”‚                             â”‚ â”‚
â”‚  â”‚                             â”‚  â”‚  These scale each sample's  â”‚ â”‚
â”‚  â”‚  Uses: ontology_set_        â”‚  â”‚  loss contribution by       â”‚ â”‚
â”‚  â”‚  similarity (Â§4.4)          â”‚  â”‚  0.5x â€“ 1.5x               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.4 Pairwise Skill Similarity (ontology_set_similarity)

To compare the skill sets of a resume and a job, CACL computes a symmetric similarity score using shortest-path distances on the ESCO graph. This is done in three steps:

**Step 1: Skill-to-skill similarity.** Given two individual skill URIs `u` and `v`, their similarity decays exponentially with their shortest-path distance on the graph:

```
skill_sim(u, v) = exp(-Î± Â· d(u, v))       where Î± = 0.7, d = shortest path length

Examples (using the graph above):
  skill_sim(Python, Java)       = exp(-0.7 Ã— 2) = 0.247   (same occupation â†’ moderate)
  skill_sim(Python, Statistics) = exp(-0.7 Ã— 2) = 0.247   (same occupation â†’ moderate)
  skill_sim(Java, Statistics)   = exp(-0.7 Ã— 4) = 0.061   (different occupations â†’ low)
  skill_sim(Python, Python)     = exp(-0.7 Ã— 0) = 1.000   (identical â†’ perfect)
  skill_sim(Python, Cooking)    = 0.000                     (no path â†’ zero)
```

The decay parameter Î± = 0.7 means similarity drops to ~0.25 at distance 2 (skills in the same occupation) and to ~0.06 at distance 4 (skills in related but different occupations). Skills with no connecting path get similarity 0.

**Step 2: Directional set score.** For a set of skills X (e.g., resume skills) and a set Y (e.g., job skills), compute how well X is "covered" by Y. For each skill in X, find its best match in Y:

```
dir_score(X â†’ Y) = (1 / |X|) Â· Î£_{x âˆˆ X} max_{y âˆˆ Y} skill_sim(x, y)
```

This measures: "on average, how well is each skill in X represented by at least one skill in Y?" A score of 1.0 means every skill in X has an identical match in Y. A score near 0 means X's skills are unrelated to anything in Y.

**Step 3: Symmetric similarity.** The final similarity is the average of both directions:

```
ontology_set_similarity(A, B) = 0.5 Â· (dir_score(A â†’ B) + dir_score(B â†’ A))
```

This is symmetric: similarity(resume_skills, job_skills) = similarity(job_skills, resume_skills). It captures both "does the candidate have the skills the job needs?" and "are the candidate's skills relevant to this job?"

### 4.5 Optimal Transport Distance (ot_distance)

While `ontology_set_similarity` uses a greedy best-match approach (each skill matches independently), the Sinkhorn Optimal Transport (OT) distance provides a more holistic comparison by finding the minimum-cost assignment between two skill sets.

**Intuition:** Imagine you need to "transport" the resume's skill distribution to match the job's skill distribution. Each unit of skill must be moved to some skill in the other set, and the cost of moving skill `u` to skill `v` is their shortest-path distance on the ESCO graph. OT finds the assignment that minimizes total transport cost.

**Computation:**

```
Given:
  A = {aâ‚, aâ‚‚, ..., aâ‚™}  (resume skill URIs)
  B = {bâ‚, bâ‚‚, ..., bâ‚˜}  (job skill URIs)

1. Build cost matrix C âˆˆ â„â¿Ë£áµ:
   C[i,j] = shortest_path(aáµ¢, bâ±¼)  on the ESCO graph
   (If no path exists, C[i,j] = 10.0 as a penalty)

2. Define uniform distributions:
   p = (1/n, 1/n, ..., 1/n)   (each resume skill has equal weight)
   q = (1/m, 1/m, ..., 1/m)   (each job skill has equal weight)

3. Solve for the optimal transport plan P* using Sinkhorn iteration:
   P* = argmin_P  Î£áµ¢â±¼ Páµ¢â±¼ Â· Cáµ¢â±¼  +  Îµ Â· KL(P || pqáµ€)

   where Îµ = 0.4 (entropic regularization) and KL is the Kullback-Leibler divergence.

4. OT distance = Î£áµ¢â±¼ P*áµ¢â±¼ Â· Cáµ¢â±¼
```

The Sinkhorn algorithm approximates the exact OT solution efficiently using iterative matrix scaling (200 iterations max, convergence tolerance 1e-6). The entropic regularization Îµ = 0.4 smooths the transport plan, making it differentiable and more robust to noise.

**Difference from ontology_set_similarity:** The greedy best-match approach can "reuse" the same skill in Y to match multiple skills in X. OT enforces that each skill contributes proportionally, giving a more balanced comparison. For example, if a resume has 5 Python-related skills and a job needs 1 Python skill + 4 management skills, the greedy approach would score high (all 5 match Python), but OT would score lower (4 skills have no good match).

**Usage in CACL:** Both `ontology_similarity` and `ot_distance` are precomputed during data preprocessing and stored in each sample's metadata. Neither is used directly as a training target or label. Instead, they serve as confidence signals in the sample-level loss weighting (Â§5.7):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         How ot_distance and ontology_similarity flow into training    â”‚
â”‚                                                                       â”‚
â”‚  Data Preprocessing (offline, before training):                       â”‚
â”‚    For each resume-job pair:                                          â”‚
â”‚      1. Map skill names â†’ ESCO skill URIs                             â”‚
â”‚      2. Compute ontology_similarity (greedy best-match, Â§4.4)         â”‚
â”‚      3. Compute ot_distance (Sinkhorn OT, Â§4.5)                      â”‚
â”‚      4. Assign quality_tier (A/B/C based on URI coverage)             â”‚
â”‚      5. Store all three in sample metadata                            â”‚
â”‚                                                                       â”‚
â”‚  During Phase 1 Training (per triplet):                               â”‚
â”‚    1. Compute InfoNCE loss as usual                                   â”‚
â”‚    2. Read ontology_similarity, ot_distance, quality_tier             â”‚
â”‚       from the anchor sample's metadata                               â”‚
â”‚    3. Combine into a single weight w âˆˆ [0.5, 1.5]                    â”‚
â”‚    4. Final loss = w Ã— InfoNCE loss                                   â”‚
â”‚                                                                       â”‚
â”‚  Effect:                                                              â”‚
â”‚    â€¢ High ontology_similarity + low ot_distance â†’ w â‰ˆ 1.5            â”‚
â”‚      (ontology confirms this is a strong match â†’ trust this sample)   â”‚
â”‚    â€¢ Low ontology_similarity + high ot_distance â†’ w â‰ˆ 0.5            â”‚
â”‚      (ontology says skills don't align â†’ downweight this sample)      â”‚
â”‚    â€¢ Missing URIs (tier C) â†’ w â‰ˆ 0.75                                â”‚
â”‚      (no ontology signal â†’ reduce influence)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The key idea: not all training samples are equally informative. Samples where the ESCO ontology agrees with the label (e.g., a positive pair with high skill overlap) should influence the model more than samples where the ontology data is missing or contradicts the label. The `ot_distance` and `ontology_similarity` together provide two complementary views of skill alignment â€” one greedy, one holistic â€” and their average forms the ontology signal used in the weight calculation.


## 5. Phase 1 â€” Contrastive Pretraining

### 5.1 Objective

Learn a shared embedding space where matching resume-job pairs have higher cosine similarity than non-matching pairs, without relying on binary labels directly. The contrastive objective forces the model to learn fine-grained semantic distinctions.

### 5.2 Model Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Phase 1 Model Architecture                     â”‚
â”‚                                                                  â”‚
â”‚  Resume Text â”€â”€â”                                                 â”‚
â”‚                â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                â”œâ”€â”€â”€â–¶â”‚  SentenceTransformerâ”‚â”€â”€â”€â–¶â”‚  Projection  â”‚â”€â”€â–¶ 128-d embedding
â”‚                â”‚    â”‚  (all-mpnet-base-v2)â”‚    â”‚  Head (MLP)  â”‚   â”‚
â”‚  Job Text â”€â”€â”€â”€â”€â”˜    â”‚  768-d output       â”‚    â”‚  768â†’256â†’128 â”‚   â”‚
â”‚                     â”‚  â„ FROZEN â„        â”‚    â”‚  ğŸ”¥ TRAINABLEâ”‚   â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  Projection Head:                                                â”‚
â”‚    Linear(768, 256) â†’ ReLU â†’ Dropout(0.3) â†’ Linear(256, 128)    â”‚
â”‚                                                                  â”‚
â”‚  Trainable params:  ~230K  (projection head only)                â”‚
â”‚  Frozen params:     ~110M  (SentenceTransformer)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The SentenceTransformer (`all-mpnet-base-v2`) is frozen to prevent catastrophic forgetting. Only the lightweight projection head is trained. This is a deliberate research design choice: the pre-trained language model already captures rich semantic representations; the projection head learns to map these into a task-specific space optimized for resume-job matching.

### 5.3 Text Encoding Priority

Content is serialized to text with a deliberate field priority to fit within the 512-token window:

```
Resume: "Position: {exp_level} {role} [SEP] Skills: {skill1, skill2, ...} [SEP] Profile: {experience_text}"
Job:    "Position: {title} [SEP] Required Skills: {skill1, skill2, ...} [SEP] Description: {desc_text}"
```

Skills are placed before experience text to guarantee they are always encoded (experience is truncated at ~800 chars).

### 5.4 Embedding Cache Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Embedding Cache Flow                        â”‚
â”‚                                                               â”‚
â”‚  Content â”€â”€â–¶ SHA-256 hash â”€â”€â–¶ Cache lookup                    â”‚
â”‚                                    â”‚                          â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                          â”‚                    â”‚               â”‚
â”‚                       HIT âœ“               MISS âœ—              â”‚
â”‚                          â”‚                    â”‚               â”‚
â”‚                   Return cached         Encode with frozen    â”‚
â”‚                   768-d text emb        SentenceTransformer   â”‚
â”‚                          â”‚                    â”‚               â”‚
â”‚                          â”‚              Store in cache         â”‚
â”‚                          â”‚                    â”‚               â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                   â”‚                           â”‚
â”‚                          Pass through trainable               â”‚
â”‚                          projection head (fresh                â”‚
â”‚                          each batch â†’ grad flows)             â”‚
â”‚                                   â”‚                           â”‚
â”‚                          128-d final embedding                â”‚
â”‚                          (with computational graph)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Key insight: only the frozen 768-d text embeddings are cached. The trainable projection head processes them fresh each batch, creating a new computational graph so gradients can flow during backpropagation. This gives the speed benefit of caching without breaking gradient flow.

### 5.5 Triplet Construction

Within each batch, only positive samples (label=1, i.e., `good_fit`) are used as anchors. Negative samples (label=0) in the batch are not used as anchors â€” they only contribute their jobs to the candidate negative pool. This means Phase 1 learns from the perspective of matching resumes: "given a resume that matches this job, learn to distinguish it from non-matching jobs."

Each anchor produces one triplet: the resume is the anchor, the matched job is the positive, and up to 7 negatives are selected from the global pool of 381 unique jobs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Contrastive Triplet                      â”‚
â”‚                                                           â”‚
â”‚  Anchor (Resume) â—„â”€â”€â”€â”€ positive pair â”€â”€â”€â”€â–º Positive (Job) â”‚
â”‚        â”‚                                                  â”‚
â”‚        â”‚â”€â”€â”€â”€ negative pairs â”€â”€â”€â”€â–º Negative Job 1          â”‚
â”‚        â”‚                         Negative Job 2           â”‚
â”‚        â”‚                         ...                      â”‚
â”‚        â”‚                         Negative Job 7           â”‚
â”‚        â”‚                    (max 7 per anchor)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

With 4,143 training samples at batch size 64, this produces ~65 batches per epoch. Since only ~26% of samples are positive, each batch yields roughly 16-18 triplets.


### 5.6 Ontology-Aware Negative Selection with Curriculum Learning

This is a core contribution of CACL. Instead of random negative sampling, negatives are selected based on their skill-level ontology distance to the anchor resume, using the ESCO knowledge graph.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Ontology-Aware Negative Selection                      â”‚
â”‚                                                                   â”‚
â”‚  For each anchor resume (with skill URIs):                        â”‚
â”‚                                                                   â”‚
â”‚  1. Compute ontology_set_similarity(resume_skills, job_skills)    â”‚
â”‚     for every candidate negative job                              â”‚
â”‚                                                                   â”‚
â”‚  2. Convert to distance: d = 1 - similarity                      â”‚
â”‚                                                                   â”‚
â”‚  3. Bucket candidates:                                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚     â”‚ Bucket   â”‚ Distance Range  â”‚ Meaning                  â”‚     â”‚
â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚     â”‚ HARD     â”‚ d â‰¤ 0.3         â”‚ Very similar skills      â”‚     â”‚
â”‚     â”‚ MEDIUM   â”‚ 0.3 < d â‰¤ 0.6  â”‚ Partially overlapping    â”‚     â”‚
â”‚     â”‚ EASY     â”‚ d > 0.6         â”‚ Very different skills    â”‚     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                   â”‚
â”‚  4. Curriculum learning shifts ratios over epochs:                 â”‚
â”‚                                                                   â”‚
â”‚     epoch_ratio = current_epoch / total_epochs  (0.0 â†’ 1.0)      â”‚
â”‚                                                                   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚     â”‚ Bucket   â”‚ Early (Îµ=0)  â”‚ Late (Îµ=1)   â”‚                    â”‚
â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â”‚
â”‚     â”‚ HARD     â”‚    20%       â”‚    60%        â”‚                    â”‚
â”‚     â”‚ MEDIUM   â”‚    30%       â”‚    30%        â”‚                    â”‚
â”‚     â”‚ EASY     â”‚    50%       â”‚    10%        â”‚                    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                   â”‚
â”‚  Rationale: Early training uses mostly easy negatives so the      â”‚
â”‚  model learns basic distinctions first. As training progresses,   â”‚
â”‚  harder negatives force the model to learn fine-grained           â”‚
â”‚  skill-level differences â€” similar to curriculum learning in       â”‚
â”‚  education.                                                       â”‚
â”‚                                                                   â”‚
â”‚  Fallback: Samples without skill URIs get random negatives.       â”‚
â”‚  The loss engine downweights these via quality tier (Â§7).         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.7 Loss Function â€” InfoNCE with Sample-Level Ontology Weighting

The loss has two components: standard InfoNCE for the contrastive objective, and a sample-level multiplicative weight based on ontology data quality.

#### InfoNCE Loss

```
L_InfoNCE = -log( exp(sim(a, pâº) / Ï„) / (exp(sim(a, pâº) / Ï„) + Î£áµ¢ exp(sim(a, náµ¢â») / Ï„)) )

where:
  a   = anchor embedding (resume)
  pâº  = positive embedding (matching job)
  náµ¢â» = negative embeddings (non-matching jobs)
  Ï„   = temperature = 0.07
  sim = cosine similarity (dot product on L2-normalized vectors)
```

The temperature Ï„=0.07 sharpens the similarity distribution, making the model more sensitive to small differences.

#### Sample-Level Ontology Weight

Each triplet's loss is scaled by a weight w âˆˆ [0.5, 1.5] based on the precomputed ontology enrichment scores:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Sample-Level Ontology Weighting                  â”‚
â”‚                                                               â”‚
â”‚  1. Base weight from quality tier:                            â”‚
â”‚     A â†’ 1.0  |  B â†’ 0.9  |  C â†’ 0.75  |  D â†’ 0.6  |  F â†’ 0.5â”‚
â”‚                                                               â”‚
â”‚  2. Ontology signal (average of available scores):            â”‚
â”‚     ont_signal = mean(ontology_similarity, 1 - ot_dist/10)   â”‚
â”‚                                                               â”‚
â”‚  3. Final weight:                                             â”‚
â”‚     w = base Ã— (1 + 0.3 Ã— (2 Ã— ont_signal - 1))             â”‚
â”‚     w = clamp(w, 0.5, 1.5)                                   â”‚
â”‚                                                               â”‚
â”‚  Effect: Samples with rich ESCO coverage and strong ontology  â”‚
â”‚  agreement contribute more to the loss. Samples with missing  â”‚
â”‚  or weak ontology data are downweighted, reducing noise.      â”‚
â”‚                                                               â”‚
â”‚  Final loss per triplet:                                      â”‚
â”‚     L = w Ã— L_InfoNCE                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.8 Training Procedure

Each epoch processes all 65 batches. After each batch:
1. Gradients are clipped to max norm 1.0 to prevent gradient explosion (large norms are logged as warnings)
2. The Adam optimizer updates only the projection head parameters

After each epoch:
1. Validation loss is computed on the held-out validation set (517 samples) using the same triplet construction and InfoNCE loss, but with no gradient updates
2. If the validation loss improves, the model is saved as `best_checkpoint.pt`
3. The best checkpoint (lowest validation loss) is passed to Phase 2

### 5.9 Training Configuration (Phase 1)

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Epochs | 15 | Val loss best at epoch 13 |
| Batch size | 64 | Balance between gradient stability and memory |
| Learning rate | 8.5e-5 | Conservative for projection head |
| Temperature | 0.07 | Sharp similarity distribution |
| Max negatives/anchor | 7 | Sufficient contrast without memory issues |
| Projection dim | 128 | Compact embedding space |
| Projection dropout | 0.3 | Regularization |
| Global neg pool | 381 unique jobs | All unique jobs from training set (deduplicated by job_id) |
| Embedding cache | Enabled, not cleared between epochs | Text encoder is frozen â†’ embeddings don't change |
| Validation | Every epoch, on held-out validation set | Early stopping via best checkpoint |


## 6. Phase 2 â€” Classification Fine-tuning

### 6.1 Objective

Use the embedding space learned in Phase 1 as a feature extractor, and train a classification head to predict binary match/no-match labels. This converts the unsupervised contrastive signal into a supervised prediction.

### 6.2 Model Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 2 Model Architecture                         â”‚
â”‚                                                                       â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  Resume Text â”€â”€â”€â”€â”€â”€â–¶â”‚ SentenceTransformerâ”‚â”€â”€â–¶ 768-d â”€â”€â”               â”‚
â”‚                     â”‚ â„ FROZEN â„       â”‚             â”‚               â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚               â”‚
â”‚                                                      â”‚               â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚               â”‚
â”‚                     â”‚ Pre-trained       â”‚             â–¼               â”‚
â”‚                     â”‚ Projection Head   â”‚â”€â”€â–¶ 128-d resume_emb        â”‚
â”‚                     â”‚ â„ FROZEN â„       â”‚                    â”        â”‚
â”‚                     â”‚ (from Phase 1)    â”‚                    â”‚        â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚        â”‚
â”‚                                                             â”‚ concat â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚ 256-d  â”‚
â”‚                     â”‚ SentenceTransformerâ”‚â”€â”€â–¶ 768-d â”€â”€â”     â”‚        â”‚
â”‚  Job Text â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ â„ FROZEN â„       â”‚             â”‚     â”‚        â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚     â”‚        â”‚
â”‚                                                      â”‚     â”‚        â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â–¼     â”‚        â”‚
â”‚                     â”‚ Pre-trained       â”‚â”€â”€â–¶ 128-d job_emb  â”‚        â”‚
â”‚                     â”‚ Projection Head   â”‚                    â”˜        â”‚
â”‚                     â”‚ â„ FROZEN â„       â”‚                    â”‚        â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚        â”‚
â”‚                                                             â–¼        â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                     â”‚         Classification Head               â”‚      â”‚
â”‚                     â”‚  ğŸ”¥ TRAINABLE (~41K params)               â”‚      â”‚
â”‚                     â”‚                                           â”‚      â”‚
â”‚                     â”‚  Linear(256, 128) â†’ ReLU â†’ Dropout(0.3)  â”‚      â”‚
â”‚                     â”‚  Linear(128, 64)  â†’ ReLU â†’ Dropout(0.3)  â”‚      â”‚
â”‚                     â”‚  Linear(64, 1)    â†’ Sigmoid              â”‚      â”‚
â”‚                     â”‚                                           â”‚      â”‚
â”‚                     â”‚  Output: P(match) âˆˆ [0, 1]               â”‚      â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                       â”‚
â”‚  Total trainable:  ~41K  (classification head only)                   â”‚
â”‚  Total frozen:     ~110M + 230K  (text encoder + projection head)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The entire Phase 1 model (SentenceTransformer + projection head) is frozen. Only the new classification head is trained. This preserves the contrastive embedding space while learning a decision boundary on top of it.

### 6.3 Class Imbalance Handling

The dataset has ~74% negative / ~26% positive samples (after binarization, where `potential_fit` and `no_fit` are both label=0). Without correction, the model collapses to predicting all-negative. CACL uses weighted BCE loss:

```
L_BCE = -(1/N) Î£áµ¢ wáµ¢ Â· [yáµ¢Â·log(Å·áµ¢) + (1-yáµ¢)Â·log(1-Å·áµ¢)]

where wáµ¢ = 2.5  if yáµ¢ = 1 (positive sample)
      wáµ¢ = 1.0  if yáµ¢ = 0 (negative sample)
```

The weight 2.5 compensates for the ~2.6:1 class imbalance ratio.

### 6.4 Training Configuration (Phase 2)

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Epochs | 10 | Loss still decreasing; more possible |
| Batch size | 32 | Smaller batches for classification stability |
| Learning rate | 5e-4 | Higher than Phase 1 (only training small head) |
| Weight decay | 0.001 | L2 regularization |
| Classification dropout | 0.3 | Prevent overfitting |
| pos_class_weight | 2.5 | Compensate ~74/26 class imbalance |
| Freeze contrastive layers | true | Preserve Phase 1 embedding space |
| Pretrained model | Phase 1 best checkpoint (epoch 13) | Best validation loss |

### 6.5 Training Procedure

Phase 2 uses all labeled samples (both positive and negative) for supervised training. Each epoch processes all 4,143 training samples in batches of 32 (~130 batches per epoch). Validation is run every epoch on the validation set (517 samples), tracking both loss and accuracy. The best checkpoint is saved based on validation loss.

Unlike Phase 1, Phase 2 does not use triplet construction, negative sampling, or the ESCO ontology. It is a straightforward binary classification task on top of frozen contrastive embeddings.

## 7. Evaluation Methodology

### 7.1 Phase 1 Evaluation (Embedding Quality)

Evaluated on the validation set (517 samples). Measures how well the contrastive embedding space separates matching from non-matching pairs using cosine similarity:

- **AUC-ROC**: Area under the ROC curve for similarity-based classification
- **Embedding separation**: Difference between mean positive and mean negative cosine similarity
- **Optimal threshold**: Similarity threshold that maximizes F1

### 7.2 Phase 2 Evaluation (Classification Performance)

Evaluated on the held-out test set (519 samples, never seen during training). The optimal classification threshold is tuned on the validation set, then applied to the test set:

- **Binary classification**: Accuracy, Precision, Recall, F1, AUC-ROC at both default (0.5) and optimal thresholds
- **Job ranking (MAP, MRR, NDCG)**: For each resume, rank all candidate jobs by predicted match probability. Measures how highly the correct job is ranked.
- **Resume ranking**: For each job, rank all candidate resumes. Measures retrieval quality from the employer's perspective.

Note: The test set contains 145 `good_fit`, 112 `potential_fit`, and 262 `no_fit` samples. Since `potential_fit` is labeled as negative (0), the model is evaluated on its ability to distinguish `good_fit` from both `potential_fit` and `no_fit`. This is a conservative evaluation â€” some "false positives" may be `potential_fit` samples that the model reasonably scores highly.

### 7.3 Results Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Results Summary                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Metric                 â”‚ Phase 1       â”‚ Phase 2               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AUC-ROC                â”‚ 0.720         â”‚ 0.799                 â”‚
â”‚ F1 Score               â”‚ 0.482         â”‚ 0.598                 â”‚
â”‚ Embedding Separation   â”‚ 0.078         â”‚ 0.249                 â”‚
â”‚ Positive Mean Sim      â”‚ 0.821         â”‚ 0.647 (probability)   â”‚
â”‚ Negative Mean Sim      â”‚ 0.743         â”‚ 0.398 (probability)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Job MAP                â”‚ â€”             â”‚ 0.260                 â”‚
â”‚ Resume MAP             â”‚ â€”             â”‚ 0.877                 â”‚
â”‚ Job NDCG               â”‚ â€”             â”‚ 0.418                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Train Loss (final)     â”‚ 0.515         â”‚ 0.721                 â”‚
â”‚ Val Loss (final)       â”‚ 0.806         â”‚ 0.748                 â”‚
â”‚ Best Val Loss          â”‚ 0.720 (ep 13) â”‚ 0.748 (ep 10)        â”‚
â”‚ Training Time          â”‚ ~10 min       â”‚ ~94 min               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 8. End-to-End Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚  1. DATA PREPARATION                                                    â”‚
â”‚     Dataset (JSONL) â”€â”€â–¶ ESCO enrichment (skill URIs, ontology scores)   â”‚
â”‚                    â”€â”€â–¶ 80/10/10 sequential split                        â”‚
â”‚                                                                         â”‚
â”‚  2. PHASE 1: CONTRASTIVE PRETRAINING                                    â”‚
â”‚     For each epoch:                                                     â”‚
â”‚       For each batch of 64 samples:                                     â”‚
â”‚         â”œâ”€ Filter positive samples (label=1) as anchors                 â”‚
â”‚         â”œâ”€ For each anchor:                                             â”‚
â”‚         â”‚   â”œâ”€ Resume = anchor, matched job = positive                  â”‚
â”‚         â”‚   â”œâ”€ Select 7 negatives from global pool (ontology-aware)     â”‚
â”‚         â”‚   â””â”€ Curriculum learning shifts hard/easy ratio               â”‚
â”‚         â”œâ”€ Encode all content â†’ 768-d (cached) â†’ 128-d (projection)    â”‚
â”‚         â”œâ”€ Compute InfoNCE loss Ã— ontology sample weight                â”‚
â”‚         â”œâ”€ Backprop through projection head only                        â”‚
â”‚         â””â”€ Gradient clipping (max norm = 1.0)                           â”‚
â”‚       Run validation on val set â†’ save best checkpoint                  â”‚
â”‚                                                                         â”‚
â”‚  3. PHASE 1 EVALUATION                                                  â”‚
â”‚     Load best checkpoint â†’ compute cosine similarities on val set       â”‚
â”‚     â†’ AUC-ROC, separation, threshold analysis                           â”‚
â”‚                                                                         â”‚
â”‚  4. PHASE 2: CLASSIFICATION FINE-TUNING                                 â”‚
â”‚     Load Phase 1 best checkpoint (frozen)                               â”‚
â”‚     For each epoch:                                                     â”‚
â”‚       For each batch of 32 labeled samples:                             â”‚
â”‚         â”œâ”€ Encode resume â†’ 768-d â†’ 128-d (frozen pipeline)              â”‚
â”‚         â”œâ”€ Encode job â†’ 768-d â†’ 128-d (frozen pipeline)                 â”‚
â”‚         â”œâ”€ Concatenate [resume_emb; job_emb] â†’ 256-d                    â”‚
â”‚         â”œâ”€ Classification head â†’ P(match)                               â”‚
â”‚         â”œâ”€ Weighted BCE loss (pos_weight=2.5)                           â”‚
â”‚         â””â”€ Backprop through classification head only                    â”‚
â”‚       Run validation on val set                                         â”‚
â”‚                                                                         â”‚
â”‚  5. PHASE 2 EVALUATION                                                  â”‚
â”‚     Load best checkpoint â†’ predict on test set                          â”‚
â”‚     â†’ Classification metrics + ranking metrics (MAP, NDCG)              â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 9. Key Design Decisions and Rationale

| Decision | Rationale |
|----------|-----------|
| Freeze SentenceTransformer | Prevents catastrophic forgetting of pre-trained language knowledge; only ~230K params trained vs ~110M frozen |
| Two-phase training | Phase 1 learns embedding geometry without label bias; Phase 2 leverages it for classification |
| Ontology-aware negatives | Random negatives are too easy; ESCO-guided selection creates informative contrasts that teach skill-level distinctions |
| Curriculum learning | Prevents early training collapse from too-hard negatives; gradually increases difficulty as the model improves |
| Cache text embeddings only | Frozen encoder produces identical outputs â†’ safe to cache. Projection head must run fresh for gradient flow |
| Global negative pool | Ensures consistent negative difficulty across batches (vs in-batch sampling which varies with batch composition) |
| Sample-level ontology weight | Downweights noisy samples (missing ESCO data) and upweights high-quality samples where ontology confirms the label |
| Weighted BCE in Phase 2 | Prevents collapse to majority-class prediction under ~74/26 class imbalance |
| Sequential data split | Avoids temporal leakage if data has any chronological ordering |
| Binary label mapping | `good_fit` â†’ 1, `potential_fit` + `no_fit` â†’ 0; conservative choice that treats borderline candidates as negative |
