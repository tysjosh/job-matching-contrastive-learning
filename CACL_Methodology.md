# Career-Aware Contrastive Learning (CACL) â€” Methodology

## 1. Problem Statement

Resume-job matching is a semantic matching problem: given a resume and a job description, predict whether the candidate is a good fit (i.e., likely to receive an interview). Traditional keyword-based approaches fail to capture the nuanced relationships between career trajectories, transferable skills, and job requirements.

CACL addresses this by learning a shared embedding space where semantically compatible resume-job pairs are pulled closer together, while incompatible pairs are pushed apart â€” guided by occupational ontology knowledge from the ESCO (European Skills, Competences, Qualifications and Occupations) framework.

## 2. System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CACL Training Pipeline                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Structured   â”‚    â”‚  ESCO KG     â”‚    â”‚  Raw Dataset          â”‚  â”‚
â”‚  â”‚  Dataset      â”‚â”€â”€â”€â–¶â”‚  Enrichment  â”‚â”€â”€â”€â–¶â”‚  (JSONL)              â”‚  â”‚
â”‚  â”‚  (Resume+Job) â”‚    â”‚  (Skill URIs)â”‚    â”‚  4,143 train samples  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                       â”‚              â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                          â”‚   80/10/10 Data Split   â”‚ â”‚
â”‚                                          â”‚  Train / Val / Test     â”‚ â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                       â”‚              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚              â”‚
â”‚                    â”‚                                  â”‚â”‚              â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚              â”‚  Phase 1   â”‚                  â”‚     Phase 2        â”‚   â”‚
â”‚              â”‚ Contrastiveâ”‚â”€â”€â”€ best ckpt â”€â”€â–¶â”‚  Classification    â”‚   â”‚
â”‚              â”‚ Pretrainingâ”‚                  â”‚  Fine-tuning       â”‚   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                    â”‚                                  â”‚              â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚              â”‚  Phase 1   â”‚                  â”‚     Phase 2        â”‚   â”‚
â”‚              â”‚ Evaluation â”‚                  â”‚  Evaluation        â”‚   â”‚
â”‚              â”‚ (Val set)  â”‚                  â”‚  (Test set)        â”‚   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3. Dataset and Data Representation

### 3.1 Dataset Overview

The dataset consists of 5,179 resume-job pairs derived from real recruitment data. Each sample was originally labeled with one of three categories by domain experts:

| Original Label | Count | Description |
|---------------|-------|-------------|
| good_fit | 1,353 | Strong match â€” candidate likely to receive interview |
| potential_fit | 1,139 | Partial match â€” candidate has some relevant qualifications |
| no_fit | 2,687 | Poor match â€” candidate lacks key requirements |

For binary classification, these are mapped as: `good_fit â†’ 1 (positive)`, `potential_fit â†’ 0 (negative)`, `no_fit â†’ 0 (negative)`. This means the negative class contains both clearly unqualified candidates and borderline candidates, which introduces label noise and makes the classification task harder.

### 3.2 Data Splits

The dataset is split sequentially (80/10/10) to avoid temporal leakage:

| Split | Total | Positive (good_fit) | Negative | Pos % | Unique Jobs |
|-------|-------|---------------------|----------|-------|-------------|
| Train | 4,143 | 1,086 | 3,057 (920 potential + 2,137 no_fit) | 26.2% | 381 |
| Validation | 517 | 122 | 395 (107 potential + 288 no_fit) | 23.6% | 149 |
| Test | 519 | 145 | 374 (112 potential + 262 no_fit) | 27.9% | 155 |

### 3.3 ESCO Enrichment

Each sample is pre-enriched with ESCO ontology data during preprocessing. Skill names from resumes and jobs are mapped to ESCO skill URIs, enabling ontology-based distance computation. 78% of training samples have both resume and job skill URIs available.

Quality tier distribution (training set): Tier A: 77.6% | Tier B: 0.4% | Tier C: 22.0%

### 3.4 Sample Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Training Sample                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Resume            â”‚           Job                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ role                   â”‚ â€¢ title                           â”‚
â”‚ â€¢ experience_level       â”‚ â€¢ description                     â”‚
â”‚ â€¢ skills [{name, uri}]   â”‚ â€¢ required_skills [{name, uri}]   â”‚
â”‚ â€¢ experience [entries]   â”‚ â€¢ skill_uris (from ESCO)          â”‚
â”‚ â€¢ skill_uris (from ESCO) â”‚                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ label: 1 (positive/match) or 0 (negative/no match)          â”‚
â”‚ metadata: quality_tier, ontology_similarity, ot_distance     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Precomputed metadata fields:
- **ontology_similarity**: Symmetric best-match skill similarity between resume and job skill URI sets (0â€“1)
- **ot_distance**: Sinkhorn optimal transport distance between skill sets on the ESCO graph
- **quality_tier**: A/B/C grade based on ESCO skill URI coverage (A = both have rich skill URIs, C = one or both missing)

## 4. ESCO Knowledge Graph Integration

The ESCO framework provides a structured occupational ontology with ~13K occupations and ~14K skills connected in a graph. CACL uses this in two ways:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ESCO Knowledge Graph                    â”‚
â”‚                                                           â”‚
â”‚    Occupation A â”€â”€â”€â”€ skill_1 â”€â”€â”€â”€ Occupation B            â”‚
â”‚         â”‚              â”‚              â”‚                    â”‚
â”‚      skill_2        skill_3        skill_4                â”‚
â”‚         â”‚              â”‚              â”‚                    â”‚
â”‚    Occupation C â”€â”€â”€â”€ skill_5 â”€â”€â”€â”€ Occupation D            â”‚
â”‚                                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Usage 1: Negative    â”‚  Usage 2: Sample-level           â”‚
â”‚  Selection (Â§5.6)     â”‚  Loss Weighting (Â§5.7)           â”‚
â”‚                       â”‚                                   â”‚
â”‚  Skill-level ontology â”‚  Precomputed ontology_similarity  â”‚
â”‚  distance determines  â”‚  and ot_distance used to weight   â”‚
â”‚  hard/medium/easy     â”‚  each sample's contribution to    â”‚
â”‚  negative buckets     â”‚  the loss (0.5x â€“ 1.5x)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### OntologySkillMatcher

Computes skill-level similarity between resume and job using shortest-path distances on the ESCO graph:

```
skill_sim(u, v) = exp(-Î± Â· shortest_path(u, v))     Î± = 0.7

ontology_set_similarity(A, B) = 0.5 Â· (dir_score(Aâ†’B) + dir_score(Bâ†’A))

where dir_score(X, Y) = (1/|X|) Â· Î£ max_yâˆˆY skill_sim(x, y)
```

This gives a 0â€“1 similarity score: 1.0 = identical skill sets, 0.0 = completely unrelated.


## 5. Phase 1 â€” Contrastive Pretraining

### 5.1 Objective

Learn a shared embedding space where matching resume-job pairs have higher cosine similarity than non-matching pairs, without relying on binary labels directly. The contrastive objective forces the model to learn fine-grained semantic distinctions.

### 5.2 Model Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Phase 1 Model Architecture                     â”‚
â”‚                                                                  â”‚
â”‚  Resume Text â”€â”€â”                                                 â”‚
â”‚                â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                â”œâ”€â”€â”€â–¶â”‚  SentenceTransformerâ”‚â”€â”€â”€â–¶â”‚  Projection  â”‚â”€â”€â–¶ 128-d embedding
â”‚                â”‚    â”‚  (all-mpnet-base-v2)â”‚    â”‚  Head (MLP)  â”‚   â”‚
â”‚  Job Text â”€â”€â”€â”€â”€â”˜    â”‚  768-d output       â”‚    â”‚  768â†’256â†’128 â”‚   â”‚
â”‚                     â”‚  â„ FROZEN â„        â”‚    â”‚  ğŸ”¥ TRAINABLEâ”‚   â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  Projection Head:                                                â”‚
â”‚    Linear(768, 256) â†’ ReLU â†’ Dropout(0.3) â†’ Linear(256, 128)    â”‚
â”‚                                                                  â”‚
â”‚  Trainable params:  ~230K  (projection head only)                â”‚
â”‚  Frozen params:     ~110M  (SentenceTransformer)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The SentenceTransformer (`all-mpnet-base-v2`) is frozen to prevent catastrophic forgetting. Only the lightweight projection head is trained. This is a deliberate research design choice: the pre-trained language model already captures rich semantic representations; the projection head learns to map these into a task-specific space optimized for resume-job matching.

### 5.3 Text Encoding Priority

Content is serialized to text with a deliberate field priority to fit within the 512-token window:

```
Resume: "Position: {exp_level} {role} [SEP] Skills: {skill1, skill2, ...} [SEP] Profile: {experience_text}"
Job:    "Position: {title} [SEP] Required Skills: {skill1, skill2, ...} [SEP] Description: {desc_text}"
```

Skills are placed before experience text to guarantee they are always encoded (experience is truncated at ~800 chars).

### 5.4 Embedding Cache Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Embedding Cache Flow                        â”‚
â”‚                                                               â”‚
â”‚  Content â”€â”€â–¶ SHA-256 hash â”€â”€â–¶ Cache lookup                    â”‚
â”‚                                    â”‚                          â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                          â”‚                    â”‚               â”‚
â”‚                       HIT âœ“               MISS âœ—              â”‚
â”‚                          â”‚                    â”‚               â”‚
â”‚                   Return cached         Encode with frozen    â”‚
â”‚                   768-d text emb        SentenceTransformer   â”‚
â”‚                          â”‚                    â”‚               â”‚
â”‚                          â”‚              Store in cache         â”‚
â”‚                          â”‚                    â”‚               â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                   â”‚                           â”‚
â”‚                          Pass through trainable               â”‚
â”‚                          projection head (fresh                â”‚
â”‚                          each batch â†’ grad flows)             â”‚
â”‚                                   â”‚                           â”‚
â”‚                          128-d final embedding                â”‚
â”‚                          (with computational graph)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Key insight: only the frozen 768-d text embeddings are cached. The trainable projection head processes them fresh each batch, creating a new computational graph so gradients can flow during backpropagation. This gives the speed benefit of caching without breaking gradient flow.

### 5.5 Triplet Construction

Within each batch, only positive samples (label=1, i.e., `good_fit`) are used as anchors. Negative samples (label=0) in the batch are not used as anchors â€” they only contribute their jobs to the candidate negative pool. This means Phase 1 learns from the perspective of matching resumes: "given a resume that matches this job, learn to distinguish it from non-matching jobs."

Each anchor produces one triplet: the resume is the anchor, the matched job is the positive, and up to 7 negatives are selected from the global pool of 381 unique jobs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Contrastive Triplet                      â”‚
â”‚                                                           â”‚
â”‚  Anchor (Resume) â—„â”€â”€â”€â”€ positive pair â”€â”€â”€â”€â–º Positive (Job) â”‚
â”‚        â”‚                                                  â”‚
â”‚        â”‚â”€â”€â”€â”€ negative pairs â”€â”€â”€â”€â–º Negative Job 1          â”‚
â”‚        â”‚                         Negative Job 2           â”‚
â”‚        â”‚                         ...                      â”‚
â”‚        â”‚                         Negative Job 7           â”‚
â”‚        â”‚                    (max 7 per anchor)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

With 4,143 training samples at batch size 64, this produces ~65 batches per epoch. Since only ~26% of samples are positive, each batch yields roughly 16-18 triplets.


### 5.6 Ontology-Aware Negative Selection with Curriculum Learning

This is a core contribution of CACL. Instead of random negative sampling, negatives are selected based on their skill-level ontology distance to the anchor resume, using the ESCO knowledge graph.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Ontology-Aware Negative Selection                      â”‚
â”‚                                                                   â”‚
â”‚  For each anchor resume (with skill URIs):                        â”‚
â”‚                                                                   â”‚
â”‚  1. Compute ontology_set_similarity(resume_skills, job_skills)    â”‚
â”‚     for every candidate negative job                              â”‚
â”‚                                                                   â”‚
â”‚  2. Convert to distance: d = 1 - similarity                      â”‚
â”‚                                                                   â”‚
â”‚  3. Bucket candidates:                                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚     â”‚ Bucket   â”‚ Distance Range  â”‚ Meaning                  â”‚     â”‚
â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚     â”‚ HARD     â”‚ d â‰¤ 0.3         â”‚ Very similar skills      â”‚     â”‚
â”‚     â”‚ MEDIUM   â”‚ 0.3 < d â‰¤ 0.6  â”‚ Partially overlapping    â”‚     â”‚
â”‚     â”‚ EASY     â”‚ d > 0.6         â”‚ Very different skills    â”‚     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                   â”‚
â”‚  4. Curriculum learning shifts ratios over epochs:                 â”‚
â”‚                                                                   â”‚
â”‚     epoch_ratio = current_epoch / total_epochs  (0.0 â†’ 1.0)      â”‚
â”‚                                                                   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚     â”‚ Bucket   â”‚ Early (Îµ=0)  â”‚ Late (Îµ=1)   â”‚                    â”‚
â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â”‚
â”‚     â”‚ HARD     â”‚    20%       â”‚    60%        â”‚                    â”‚
â”‚     â”‚ MEDIUM   â”‚    30%       â”‚    30%        â”‚                    â”‚
â”‚     â”‚ EASY     â”‚    50%       â”‚    10%        â”‚                    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                   â”‚
â”‚  Rationale: Early training uses mostly easy negatives so the      â”‚
â”‚  model learns basic distinctions first. As training progresses,   â”‚
â”‚  harder negatives force the model to learn fine-grained           â”‚
â”‚  skill-level differences â€” similar to curriculum learning in       â”‚
â”‚  education.                                                       â”‚
â”‚                                                                   â”‚
â”‚  Fallback: Samples without skill URIs get random negatives.       â”‚
â”‚  The loss engine downweights these via quality tier (Â§7).         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.7 Loss Function â€” InfoNCE with Sample-Level Ontology Weighting

The loss has two components: standard InfoNCE for the contrastive objective, and a sample-level multiplicative weight based on ontology data quality.

#### InfoNCE Loss

```
L_InfoNCE = -log( exp(sim(a, pâº) / Ï„) / (exp(sim(a, pâº) / Ï„) + Î£áµ¢ exp(sim(a, náµ¢â») / Ï„)) )

where:
  a   = anchor embedding (resume)
  pâº  = positive embedding (matching job)
  náµ¢â» = negative embeddings (non-matching jobs)
  Ï„   = temperature = 0.07
  sim = cosine similarity (dot product on L2-normalized vectors)
```

The temperature Ï„=0.07 sharpens the similarity distribution, making the model more sensitive to small differences.

#### Sample-Level Ontology Weight

Each triplet's loss is scaled by a weight w âˆˆ [0.5, 1.5] based on the precomputed ontology enrichment scores:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Sample-Level Ontology Weighting                  â”‚
â”‚                                                               â”‚
â”‚  1. Base weight from quality tier:                            â”‚
â”‚     A â†’ 1.0  |  B â†’ 0.9  |  C â†’ 0.75  |  D â†’ 0.6  |  F â†’ 0.5â”‚
â”‚                                                               â”‚
â”‚  2. Ontology signal (average of available scores):            â”‚
â”‚     ont_signal = mean(ontology_similarity, 1 - ot_dist/10)   â”‚
â”‚                                                               â”‚
â”‚  3. Final weight:                                             â”‚
â”‚     w = base Ã— (1 + 0.3 Ã— (2 Ã— ont_signal - 1))             â”‚
â”‚     w = clamp(w, 0.5, 1.5)                                   â”‚
â”‚                                                               â”‚
â”‚  Effect: Samples with rich ESCO coverage and strong ontology  â”‚
â”‚  agreement contribute more to the loss. Samples with missing  â”‚
â”‚  or weak ontology data are downweighted, reducing noise.      â”‚
â”‚                                                               â”‚
â”‚  Final loss per triplet:                                      â”‚
â”‚     L = w Ã— L_InfoNCE                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.8 Training Procedure

Each epoch processes all 65 batches. After each batch:
1. Gradients are clipped to max norm 1.0 to prevent gradient explosion (large norms are logged as warnings)
2. The Adam optimizer updates only the projection head parameters

After each epoch:
1. Validation loss is computed on the held-out validation set (517 samples) using the same triplet construction and InfoNCE loss, but with no gradient updates
2. If the validation loss improves, the model is saved as `best_checkpoint.pt`
3. The best checkpoint (lowest validation loss) is passed to Phase 2

### 5.9 Training Configuration (Phase 1)

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Epochs | 15 | Val loss best at epoch 13 |
| Batch size | 64 | Balance between gradient stability and memory |
| Learning rate | 8.5e-5 | Conservative for projection head |
| Temperature | 0.07 | Sharp similarity distribution |
| Max negatives/anchor | 7 | Sufficient contrast without memory issues |
| Projection dim | 128 | Compact embedding space |
| Projection dropout | 0.3 | Regularization |
| Global neg pool | 381 unique jobs | All unique jobs from training set (deduplicated by job_id) |
| Embedding cache | Enabled, not cleared between epochs | Text encoder is frozen â†’ embeddings don't change |
| Validation | Every epoch, on held-out validation set | Early stopping via best checkpoint |


## 6. Phase 2 â€” Classification Fine-tuning

### 6.1 Objective

Use the embedding space learned in Phase 1 as a feature extractor, and train a classification head to predict binary match/no-match labels. This converts the unsupervised contrastive signal into a supervised prediction.

### 6.2 Model Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 2 Model Architecture                         â”‚
â”‚                                                                       â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  Resume Text â”€â”€â”€â”€â”€â”€â–¶â”‚ SentenceTransformerâ”‚â”€â”€â–¶ 768-d â”€â”€â”               â”‚
â”‚                     â”‚ â„ FROZEN â„       â”‚             â”‚               â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚               â”‚
â”‚                                                      â”‚               â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚               â”‚
â”‚                     â”‚ Pre-trained       â”‚             â–¼               â”‚
â”‚                     â”‚ Projection Head   â”‚â”€â”€â–¶ 128-d resume_emb        â”‚
â”‚                     â”‚ â„ FROZEN â„       â”‚                    â”        â”‚
â”‚                     â”‚ (from Phase 1)    â”‚                    â”‚        â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚        â”‚
â”‚                                                             â”‚ concat â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚ 256-d  â”‚
â”‚                     â”‚ SentenceTransformerâ”‚â”€â”€â–¶ 768-d â”€â”€â”     â”‚        â”‚
â”‚  Job Text â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ â„ FROZEN â„       â”‚             â”‚     â”‚        â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚     â”‚        â”‚
â”‚                                                      â”‚     â”‚        â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â–¼     â”‚        â”‚
â”‚                     â”‚ Pre-trained       â”‚â”€â”€â–¶ 128-d job_emb  â”‚        â”‚
â”‚                     â”‚ Projection Head   â”‚                    â”˜        â”‚
â”‚                     â”‚ â„ FROZEN â„       â”‚                    â”‚        â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚        â”‚
â”‚                                                             â–¼        â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                     â”‚         Classification Head               â”‚      â”‚
â”‚                     â”‚  ğŸ”¥ TRAINABLE (~41K params)               â”‚      â”‚
â”‚                     â”‚                                           â”‚      â”‚
â”‚                     â”‚  Linear(256, 128) â†’ ReLU â†’ Dropout(0.3)  â”‚      â”‚
â”‚                     â”‚  Linear(128, 64)  â†’ ReLU â†’ Dropout(0.3)  â”‚      â”‚
â”‚                     â”‚  Linear(64, 1)    â†’ Sigmoid              â”‚      â”‚
â”‚                     â”‚                                           â”‚      â”‚
â”‚                     â”‚  Output: P(match) âˆˆ [0, 1]               â”‚      â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                       â”‚
â”‚  Total trainable:  ~41K  (classification head only)                   â”‚
â”‚  Total frozen:     ~110M + 230K  (text encoder + projection head)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The entire Phase 1 model (SentenceTransformer + projection head) is frozen. Only the new classification head is trained. This preserves the contrastive embedding space while learning a decision boundary on top of it.

### 6.3 Class Imbalance Handling

The dataset has ~74% negative / ~26% positive samples (after binarization, where `potential_fit` and `no_fit` are both label=0). Without correction, the model collapses to predicting all-negative. CACL uses weighted BCE loss:

```
L_BCE = -(1/N) Î£áµ¢ wáµ¢ Â· [yáµ¢Â·log(Å·áµ¢) + (1-yáµ¢)Â·log(1-Å·áµ¢)]

where wáµ¢ = 2.5  if yáµ¢ = 1 (positive sample)
      wáµ¢ = 1.0  if yáµ¢ = 0 (negative sample)
```

The weight 2.5 compensates for the ~2.6:1 class imbalance ratio.

### 6.4 Training Configuration (Phase 2)

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Epochs | 10 | Loss still decreasing; more possible |
| Batch size | 32 | Smaller batches for classification stability |
| Learning rate | 5e-4 | Higher than Phase 1 (only training small head) |
| Weight decay | 0.001 | L2 regularization |
| Classification dropout | 0.3 | Prevent overfitting |
| pos_class_weight | 2.5 | Compensate ~74/26 class imbalance |
| Freeze contrastive layers | true | Preserve Phase 1 embedding space |
| Pretrained model | Phase 1 best checkpoint (epoch 13) | Best validation loss |

### 6.5 Training Procedure

Phase 2 uses all labeled samples (both positive and negative) for supervised training. Each epoch processes all 4,143 training samples in batches of 32 (~130 batches per epoch). Validation is run every epoch on the validation set (517 samples), tracking both loss and accuracy. The best checkpoint is saved based on validation loss.

Unlike Phase 1, Phase 2 does not use triplet construction, negative sampling, or the ESCO ontology. It is a straightforward binary classification task on top of frozen contrastive embeddings.

## 7. Evaluation Methodology

### 7.1 Phase 1 Evaluation (Embedding Quality)

Evaluated on the validation set (517 samples). Measures how well the contrastive embedding space separates matching from non-matching pairs using cosine similarity:

- **AUC-ROC**: Area under the ROC curve for similarity-based classification
- **Embedding separation**: Difference between mean positive and mean negative cosine similarity
- **Optimal threshold**: Similarity threshold that maximizes F1

### 7.2 Phase 2 Evaluation (Classification Performance)

Evaluated on the held-out test set (519 samples, never seen during training). The optimal classification threshold is tuned on the validation set, then applied to the test set:

- **Binary classification**: Accuracy, Precision, Recall, F1, AUC-ROC at both default (0.5) and optimal thresholds
- **Job ranking (MAP, MRR, NDCG)**: For each resume, rank all candidate jobs by predicted match probability. Measures how highly the correct job is ranked.
- **Resume ranking**: For each job, rank all candidate resumes. Measures retrieval quality from the employer's perspective.

Note: The test set contains 145 `good_fit`, 112 `potential_fit`, and 262 `no_fit` samples. Since `potential_fit` is labeled as negative (0), the model is evaluated on its ability to distinguish `good_fit` from both `potential_fit` and `no_fit`. This is a conservative evaluation â€” some "false positives" may be `potential_fit` samples that the model reasonably scores highly.

### 7.3 Results Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Results Summary                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Metric                 â”‚ Phase 1       â”‚ Phase 2               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AUC-ROC                â”‚ 0.720         â”‚ 0.799                 â”‚
â”‚ F1 Score               â”‚ 0.482         â”‚ 0.598                 â”‚
â”‚ Embedding Separation   â”‚ 0.078         â”‚ 0.249                 â”‚
â”‚ Positive Mean Sim      â”‚ 0.821         â”‚ 0.647 (probability)   â”‚
â”‚ Negative Mean Sim      â”‚ 0.743         â”‚ 0.398 (probability)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Job MAP                â”‚ â€”             â”‚ 0.260                 â”‚
â”‚ Resume MAP             â”‚ â€”             â”‚ 0.877                 â”‚
â”‚ Job NDCG               â”‚ â€”             â”‚ 0.418                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Train Loss (final)     â”‚ 0.515         â”‚ 0.721                 â”‚
â”‚ Val Loss (final)       â”‚ 0.806         â”‚ 0.748                 â”‚
â”‚ Best Val Loss          â”‚ 0.720 (ep 13) â”‚ 0.748 (ep 10)        â”‚
â”‚ Training Time          â”‚ ~10 min       â”‚ ~94 min               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 8. End-to-End Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚  1. DATA PREPARATION                                                    â”‚
â”‚     Dataset (JSONL) â”€â”€â–¶ ESCO enrichment (skill URIs, ontology scores)   â”‚
â”‚                    â”€â”€â–¶ 80/10/10 sequential split                        â”‚
â”‚                                                                         â”‚
â”‚  2. PHASE 1: CONTRASTIVE PRETRAINING                                    â”‚
â”‚     For each epoch:                                                     â”‚
â”‚       For each batch of 64 samples:                                     â”‚
â”‚         â”œâ”€ Filter positive samples (label=1) as anchors                 â”‚
â”‚         â”œâ”€ For each anchor:                                             â”‚
â”‚         â”‚   â”œâ”€ Resume = anchor, matched job = positive                  â”‚
â”‚         â”‚   â”œâ”€ Select 7 negatives from global pool (ontology-aware)     â”‚
â”‚         â”‚   â””â”€ Curriculum learning shifts hard/easy ratio               â”‚
â”‚         â”œâ”€ Encode all content â†’ 768-d (cached) â†’ 128-d (projection)    â”‚
â”‚         â”œâ”€ Compute InfoNCE loss Ã— ontology sample weight                â”‚
â”‚         â”œâ”€ Backprop through projection head only                        â”‚
â”‚         â””â”€ Gradient clipping (max norm = 1.0)                           â”‚
â”‚       Run validation on val set â†’ save best checkpoint                  â”‚
â”‚                                                                         â”‚
â”‚  3. PHASE 1 EVALUATION                                                  â”‚
â”‚     Load best checkpoint â†’ compute cosine similarities on val set       â”‚
â”‚     â†’ AUC-ROC, separation, threshold analysis                           â”‚
â”‚                                                                         â”‚
â”‚  4. PHASE 2: CLASSIFICATION FINE-TUNING                                 â”‚
â”‚     Load Phase 1 best checkpoint (frozen)                               â”‚
â”‚     For each epoch:                                                     â”‚
â”‚       For each batch of 32 labeled samples:                             â”‚
â”‚         â”œâ”€ Encode resume â†’ 768-d â†’ 128-d (frozen pipeline)              â”‚
â”‚         â”œâ”€ Encode job â†’ 768-d â†’ 128-d (frozen pipeline)                 â”‚
â”‚         â”œâ”€ Concatenate [resume_emb; job_emb] â†’ 256-d                    â”‚
â”‚         â”œâ”€ Classification head â†’ P(match)                               â”‚
â”‚         â”œâ”€ Weighted BCE loss (pos_weight=2.5)                           â”‚
â”‚         â””â”€ Backprop through classification head only                    â”‚
â”‚       Run validation on val set                                         â”‚
â”‚                                                                         â”‚
â”‚  5. PHASE 2 EVALUATION                                                  â”‚
â”‚     Load best checkpoint â†’ predict on test set                          â”‚
â”‚     â†’ Classification metrics + ranking metrics (MAP, NDCG)              â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 9. Key Design Decisions and Rationale

| Decision | Rationale |
|----------|-----------|
| Freeze SentenceTransformer | Prevents catastrophic forgetting of pre-trained language knowledge; only ~230K params trained vs ~110M frozen |
| Two-phase training | Phase 1 learns embedding geometry without label bias; Phase 2 leverages it for classification |
| Ontology-aware negatives | Random negatives are too easy; ESCO-guided selection creates informative contrasts that teach skill-level distinctions |
| Curriculum learning | Prevents early training collapse from too-hard negatives; gradually increases difficulty as the model improves |
| Cache text embeddings only | Frozen encoder produces identical outputs â†’ safe to cache. Projection head must run fresh for gradient flow |
| Global negative pool | Ensures consistent negative difficulty across batches (vs in-batch sampling which varies with batch composition) |
| Sample-level ontology weight | Downweights noisy samples (missing ESCO data) and upweights high-quality samples where ontology confirms the label |
| Weighted BCE in Phase 2 | Prevents collapse to majority-class prediction under ~74/26 class imbalance |
| Sequential data split | Avoids temporal leakage if data has any chronological ordering |
| Binary label mapping | `good_fit` â†’ 1, `potential_fit` + `no_fit` â†’ 0; conservative choice that treats borderline candidates as negative |
