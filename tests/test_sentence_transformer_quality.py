#!/usr/bin/env python3
"""
Test to prove SentenceTransformer already provides excellent resume-job matching
without any additional contrastive training.
"""

import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

def test_sentence_transformer_quality():
    """Test SentenceTransformer's built-in ability to match resumes and jobs."""
    
    print("üéØ Testing SentenceTransformer Quality for Resume-Job Matching")
    print("=" * 65)
    
    # Load the same model you're using
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # Test cases: resume-job pairs
    test_cases = [
        {
            "resume": "Python developer with 5 years experience in machine learning and data science",
            "positive_job": "Senior Python Developer - ML focus, 5+ years experience required",
            "negative_jobs": [
                "Chef position at fine dining restaurant",
                "Medical doctor for hospital emergency department", 
                "Elementary school teacher position",
                "Java Developer - Enterprise applications, no ML experience"
            ]
        },
        {
            "resume": "Experienced chef with culinary arts degree and restaurant management skills",
            "positive_job": "Head Chef position at upscale restaurant, culinary degree preferred",
            "negative_jobs": [
                "Python software engineer position",
                "Data scientist role requiring PhD",
                "Mechanical engineer for automotive industry",
                "Marketing manager for tech startup"
            ]
        },
        {
            "resume": "Medical doctor with 10 years emergency medicine experience",
            "positive_job": "Emergency Medicine Physician - Level 1 trauma center",
            "negative_jobs": [
                "Software developer position",
                "Restaurant chef opening",
                "Marketing coordinator role",
                "Mechanical engineer position"
            ]
        }
    ]
    
    print("üîç Testing SentenceTransformer's Built-in Matching Ability")
    print("-" * 55)
    
    total_correct = 0
    total_tests = len(test_cases)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nTest {i}:")
        print(f"Resume: {test_case['resume'][:60]}...")
        print(f"Positive Job: {test_case['positive_job'][:60]}...")
        
        # Encode all texts
        resume_emb = model.encode([test_case['resume']])
        positive_emb = model.encode([test_case['positive_job']])
        negative_embs = model.encode(test_case['negative_jobs'])
        
        # Calculate similarities
        positive_sim = cosine_similarity(resume_emb, positive_emb)[0][0]
        negative_sims = cosine_similarity(resume_emb, negative_embs)[0]
        
        print(f"Positive similarity: {positive_sim:.4f}")
        print(f"Negative similarities: {[f'{sim:.4f}' for sim in negative_sims]}")
        
        # Check if positive is highest
        max_negative_sim = max(negative_sims)
        is_correct = positive_sim > max_negative_sim
        
        print(f"‚úÖ Correct ranking: {is_correct} (positive > all negatives)")
        
        if is_correct:
            total_correct += 1
            
        # Show margin
        margin = positive_sim - max_negative_sim
        print(f"Margin: {margin:.4f}")
    
    accuracy = total_correct / total_tests
    print(f"\nüìä RESULTS:")
    print(f"Accuracy: {accuracy:.1%} ({total_correct}/{total_tests})")
    
    if accuracy >= 0.8:
        print("üéâ EXCELLENT: SentenceTransformer already provides great matching!")
        print("üí° INSIGHT: Adding another model on top is likely to HURT performance")
    elif accuracy >= 0.6:
        print("‚úÖ GOOD: SentenceTransformer provides decent matching")
        print("üí° INSIGHT: Fine-tuning might help, but double encoding is risky")
    else:
        print("‚ö†Ô∏è POOR: SentenceTransformer struggles with this task")
        print("üí° INSIGHT: Fine-tuning or domain adaptation might be needed")
    
    return accuracy

def demonstrate_double_encoding_problem():
    """Show why double encoding is problematic."""
    
    print("\n" + "=" * 65)
    print("üö® Why Double Encoding is Problematic")
    print("=" * 65)
    
    print("\nüéØ The Core Issue:")
    print("-" * 20)
    print("SentenceTransformer is ALREADY trained with contrastive learning!")
    print("‚Ä¢ Trained on 1+ billion sentence pairs")
    print("‚Ä¢ Uses InfoNCE loss (same as your contrastive loss)")
    print("‚Ä¢ Optimized for semantic similarity tasks")
    print("‚Ä¢ Already understands resume-job matching")
    
    print("\nüî¥ What Your Double Encoding Does:")
    print("-" * 35)
    print("1. Takes perfect embeddings from SentenceTransformer")
    print("2. Passes them through ANOTHER neural network")
    print("3. Trains this network on tiny dataset (8,572 samples)")
    print("4. Risks destroying the pre-trained knowledge")
    
    print("\nüìä Parameter Comparison:")
    print("-" * 25)
    print("SentenceTransformer parameters: ~22,000,000")
    print("Your additional model parameters: ~500,000") 
    print("Your training samples: 8,572")
    print("Samples per parameter: 8,572 / 500,000 = 0.017")
    print("‚ùå SEVERE OVERFITTING RISK!")
    
    print("\n‚úÖ Better Approaches:")
    print("-" * 20)
    print("Option 1: Use SentenceTransformer directly (simplest)")
    print("Option 2: Fine-tune SentenceTransformer (freeze most layers)")
    print("Option 3: Add tiny projection head (64-128 dims max)")
    print("Option 4: Use adapter layers (parameter-efficient)")

def recommend_architecture_fix():
    """Recommend the best architecture for your use case."""
    
    print("\n" + "=" * 65)
    print("üõ†Ô∏è RECOMMENDED ARCHITECTURE FIX")
    print("=" * 65)
    
    print("\nüéØ Option 1: Direct SentenceTransformer (RECOMMENDED)")
    print("-" * 50)
    print("```python")
    print("class SimplifiedTrainer:")
    print("    def __init__(self, config):")
    print("        self.encoder = SentenceTransformer(config.model_name)")
    print("        # NO additional model!")
    print("    ")
    print("    def encode(self, text):")
    print("        return self.encoder.encode(text)  # Direct usage")
    print("```")
    print("Benefits:")
    print("‚Ä¢ Zero overfitting risk")
    print("‚Ä¢ Fastest training")
    print("‚Ä¢ Best generalization")
    print("‚Ä¢ Simplest code")
    
    print("\nüéØ Option 2: Tiny Projection Head (IF NEEDED)")
    print("-" * 45)
    print("```python")
    print("class MinimalProjection(nn.Module):")
    print("    def __init__(self, input_dim=384, output_dim=128):")
    print("        super().__init__()")
    print("        # Freeze SentenceTransformer")
    print("        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')")
    print("        for param in self.encoder.parameters():")
    print("            param.requires_grad = False")
    print("        ")
    print("        # Tiny projection (only 49K parameters)")
    print("        self.projection = nn.Linear(input_dim, output_dim)")
    print("    ")
    print("    def forward(self, text):")
    print("        with torch.no_grad():")
    print("            base_emb = self.encoder.encode(text)")
    print("        return F.normalize(self.projection(base_emb), p=2, dim=-1)")
    print("```")
    print("Benefits:")
    print("‚Ä¢ Preserves pre-trained knowledge")
    print("‚Ä¢ Minimal overfitting risk")
    print("‚Ä¢ Task-specific adaptation")
    
    print("\n‚ùå What NOT to Do (Your Current Approach):")
    print("-" * 45)
    print("‚Ä¢ Train both SentenceTransformer AND additional model")
    print("‚Ä¢ Use large additional networks (500K+ parameters)")
    print("‚Ä¢ Complex architectures on small datasets")

if __name__ == "__main__":
    accuracy = test_sentence_transformer_quality()
    demonstrate_double_encoding_problem()
    recommend_architecture_fix()
    
    print(f"\nüéØ CONCLUSION:")
    print(f"With {accuracy:.1%} accuracy out-of-the-box, SentenceTransformer")
    print(f"likely doesn't need additional training for your task!")